%pip install --upgrade pip
%pip install scikit-learn
%pip install nltk

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import string
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
nltk.download('stopwords')
nltk.download('wordnet')

df1= pd.read_csv('Sentiment_Analysis.csv') #load data
df1.head()

#Explore and Clean Data
print(df1.info())
print(df1['sentiment'].value_counts())
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def clean_text(text):
    text = text.lower()
    text = re.sub(r"http\S+|www\S+|https\S+", '', text, flags=re.MULTILINE)
    text = re.sub(r'\@w+|\#','', text)
    text = text.translate(str.maketrans('', '', string.punctuation))
    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])
    return text
df1['cleaned_text'] = df1['content'].apply(clean_text)
df1.head()

vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2), min_df=3)
X_vec = vectorizer.fit_transform(df['cleaned'])
y = df['sentiment']

# Balance classes with SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_vec, y)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Linear SVM for high-dimensional text
model = LinearSVC()
model.fit(X_train, y_train)
feature_names = vectorizer.get_feature_names_out()
coeff = model.coef_[0]

# Top Positive and Negative Words (recompute indices for 5000 features)
top_pos = np.argsort(coeff)[-10:]
top_neg = np.argsort(coeff)[:10]
print("Top Positive Words:", feature_names[top_pos])
print("Top Negative Words:", feature_names[top_neg])
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# Evaluation
y_pred = model.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(" Accuracy:", round(acc * 100, 2), "%")
print("\n Classification Report:\n", classification_report(y_test, y_pred, zero_division=0))

from sklearn.metrics import accuracy_score
# y_test: actual labels
# y_pred: predicted labels from your model
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy * 100:.2f}%")

# Ensure vectorizer 
vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2), min_df=3)
X_vec = vectorizer.fit_transform(df1['cleaned_text'])
y = df1['sentiment']

model = LogisticRegression(max_iter=1000)
model.fit(X_vec, y)
print(df1['sentiment'].unique())
def predict_sentiment(new_text):
    cleaned = clean_text(new_text)
    vec = vectorizer.transform([cleaned])
    pred = model.predict(vec)[0]
    return pred  

#according to the given new command,the analysis are given as output
new_command = "i had a great day at the park, it was so much fun!"
result = predict_sentiment(new_command)
print(f"Sentiment: {result}")

#visualization 
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d')

# Sentiment Classification
df.info()
df['sentiment'].value_counts()
df['sentiment'].value_counts().plot(kind='bar', color=['green', 'red', 'yellow'],alpha=0.7)

#scatterplot
plt.figure(figsize=(8,6))
sns.scatterplot(x=X_embedded[:,0], y=X_embedded[:,1], hue=sentiment_subset, palette='tab10')
plt.title('t-SNE Visualization of Text Embeddings')
plt.xlabel('Component 1')
plt.ylabel('Component 2')
plt.legend(title='Sentiment')

#visualize the most frequently used words within each sentiment category
from wordcloud import WordCloud
text_happy = " ".join(df[df['sentiment'] == 'happiness']['cleaned'])
wc = WordCloud(width=800, height=400, background_color='white').generate(text_happy)
plt.figure(figsize=(10, 5))
plt.imshow(wc, interpolation='bilinear')
plt.axis('off')
plt.title("Most Common Words - Happiness")
plt.show()
plt.show()
